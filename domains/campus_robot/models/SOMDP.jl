using Combinatorics
using Statistics
using Random
using TimerOutputs
# using ProfileView

import Base.==

include("MDP.jl")
include(joinpath(@__DIR__, "..", "..", "..", "solvers", "VIMDPSolver.jl"))
include(joinpath(@__DIR__, "..", "..", "..", "solvers", "LAOStarSolver.jl"))
include(joinpath(@__DIR__, "..", "..", "..", "solvers", "UCTSolverMDP.jl"))
include(joinpath(@__DIR__, "..", "..", "..", "solvers", "MCTSSolver.jl"))
include(joinpath(@__DIR__, "..", "..", "..", "solvers", "FLARESSolver.jl"))


function index(element, collection)
    for i=1:length(collection)
        if collection[i] == element
            return i
        end
    end
    return -1
end

struct MemoryState
    state::DomainState
    action_list::Vector{DomainAction}
end

function ==(a::MemoryState, b::MemoryState)
    return (a.state == b.state && a.action_list == b.action_list)
end

function Base.hash(a::MemoryState, h::UInt)
    h = hash(a.state, h)
    for act ‚àà a.action_list
        h = hash(act, h)
    end
    return h
end

struct MemoryAction
    value::Union{String,Char}
end

function Base.hash(a::MemoryAction, h::UInt)
    return hash(a.value, h)
end

function ==(a::MemoryAction, b::MemoryAction)
    return isequal(a.value, b.value)
end

struct SOMDP
    M::MDP
    S::Vector{MemoryState}
    A::Vector{MemoryAction}
    T::Dict{Int, Dict{Int, Vector{Tuple{Int, Float64}}}}
    R::Function
   s‚ÇÄ::MemoryState
    Œ¥::Integer
    H::Function
    Sindex::Dict{MemoryState, Integer}
    Aindex::Dict{MemoryAction, Integer}
end
function SOMDP(M::MDP,
               S::Vector{MemoryState},
               A::Vector{MemoryAction},
               T::Dict{Int, Dict{Int, Vector{Tuple{Int, Float64}}}},
               R::Function,
               s‚ÇÄ::MemoryState,
               Œ¥::Integer,
               H::Function)

    Aindex, Sindex = generate_index_dicts(A, S)
    ‚Ñ≥ = SOMDP(M, S, A, T, R, s‚ÇÄ, Œ¥, H, Sindex, Aindex)
end

function generate_index_dicts(A::Vector{MemoryAction}, S::Vector{MemoryState})
    Aindex = Dict{MemoryAction, Integer}()
    for (a, action) ‚àà enumerate(A)
        Aindex[action] = a
    end
    Sindex = Dict{MemoryState, Int64}()
    for (s, state) ‚àà enumerate(S)
        Sindex[state] = s
    end
    return Aindex, Sindex
end

function generate_states(M::MDP, Œ¥::Integer)
    A = M.A

    S = Vector{MemoryState}()
    s‚ÇÄ = -1
    for depth in 0:Œ¥
        for (i, state) in enumerate(M.S)
            if depth == 0
                s = MemoryState(state, Vector{DomainAction}())
                push!(S, s)
                if state == M.s‚ÇÄ
                    s‚ÇÄ = length(S)
                end
            else
                for action_list ‚àà collect(Base.product(ntuple(i->A, depth)...))
                    s = MemoryState(state, [a for a ‚àà action_list])
                    push!(S, s)
                end
            end
        end
    end
    # push!(S, MemoryState(DomainState(-1, -1, '‚àÖ', '‚àÖ'), DomainAction[]))
    return S, S[s‚ÇÄ]
end

function terminal(‚Ñ≥::SOMDP, state::MemoryState)
    return terminal(state.state, ‚Ñ≥.M.g)
end

function generate_actions(M::MDP)
    A = [MemoryAction(a.value) for a in M.A]
    push!(A, MemoryAction("QUERY"))
    return A
end

function eta(state::DomainState)
    # return 1.0
    ## two_buildings_map
    x, y = state.x, state.y
    if 9 >= x >= 4 && 14 >= y >= 12
        return 0.1
    elseif x == 6 && y == 18
        return 0.1
    elseif 8 >= x >= 4 && 23 >= y >= 22
        return 0.1
    else
        return 0.9
    end
end

function eta(state::MemoryState)
    return eta(state.state)
end

function eta(action::MemoryAction,
              state::MemoryState)
    return eta(state)
end

function generate_transitions(‚Ñ≥::SOMDP, incremental::Bool=false)
    M, S, A, T, Œ¥ = ‚Ñ≥.M, ‚Ñ≥.S, ‚Ñ≥.A, ‚Ñ≥.T, ‚Ñ≥.Œ¥
    for (s, state) in enumerate(S)
        if incremental && length(state.action_list) < Œ¥ - 1
            continue
        end
        T[s] = Dict{Int, Vector{Pair{Int, Float64}}}()
        for (a, action) in Iterators.reverse(enumerate(A))
            T[s][a] = generate_transitions(‚Ñ≥, s, a)
        end
    end
end

function check_transition_validity(‚Ñ≥::SOMDP)
    M, S, A, T = ‚Ñ≥.M, ‚Ñ≥.S, ‚Ñ≥.A, ‚Ñ≥.T
    for (s, state) in enumerate(S)
        for (a, action) in enumerate(A)
            mass = 0.0
            for (s‚Ä≤, p) in T[s][a]
                mass += p
            end
            if round(mass; digits=4) != 1.0
                println("Transition error at state $state and action $action.")
                println("State index: $s      Action index: $a")
                println("Total probability mass of $mass.")
                println("Transition vector is the following: \n $(T[s][a])")
                @assert false
            end
        end
    end
end

function generate_transitions(‚Ñ≥::SOMDP, s::Int, a::Int)
    M, S, A = ‚Ñ≥.M, ‚Ñ≥.S, ‚Ñ≥.A
    state, action = S[s], A[a]
    if state.state.x == -1
        return [(s, 1.0)]
    end

    T = Vector{Tuple{Int, Float64}}()
    # Inside a domain state
    if isempty(state.action_list)
        if action.value == "QUERY" # Do nothing
            return [(s, 1.0)]
        else
            i = argmax(M.T[s][a])
            if M.T[i] == 1.0
                return [(i, 1.0)]
            else
                mass = 0.0
                for (s‚Ä≤, state‚Ä≤) in enumerate(M.S)
                    p = M.T[s][a][s‚Ä≤]
                    if p == 0.0
                        continue
                    end
                    p *= eta(state‚Ä≤)
                    mass += p
                    push!(T, (s‚Ä≤, round(p; digits=5)))
                end
                ms‚Ä≤ = length(M.S) + length(M.A) * (s-1) + a
                mem_p = 1.0 - round(mass; digits=5)
                if mem_p != 0.0
                    push!(T, (ms‚Ä≤, mem_p))
                end
            end
        end
    elseif action.value == "QUERY"  # Here and below is in memory state
        prev_action = MemoryAction(last(state.action_list).value)
        p_a = ‚Ñ≥.Aindex[prev_action]
        prev_state = MemoryState(state.state,
                      state.action_list[1:length(state.action_list) - 1])
        p_s = ‚Ñ≥.Sindex[prev_state]

        len = length(‚Ñ≥.M.S)
        tmp = Dict{Int, Float64}()
        for (bs, b) in ‚Ñ≥.T[p_s][a]
            for (bs‚Ä≤, b‚Ä≤) in ‚Ñ≥.T[bs][p_a]
                if bs‚Ä≤ > len
                    continue
                end
                if !haskey(tmp, bs‚Ä≤)
                    tmp[bs‚Ä≤] = 0.0
                end
                tmp[bs‚Ä≤] += b * ‚Ñ≥.M.T[bs][p_a][bs‚Ä≤]
            end
        end
        for k in keys(tmp)
            push!(T, (k, round(tmp[k]; digits=5)))
        end
    elseif length(state.action_list) == ‚Ñ≥.Œ¥
        return [(length(‚Ñ≥.S), 1.0)]
    else # Taking non-query action in memory state before depth Œ¥ is reached
        action_list‚Ä≤ = [action for action in state.action_list]
        push!(action_list‚Ä≤, DomainAction(action.value))
        mstate‚Ä≤ = MemoryState(state.state, action_list‚Ä≤)
        ms‚Ä≤ = ‚Ñ≥.Sindex[mstate‚Ä≤]

        tmp = Dict{Int, Float64}()
        len = length(‚Ñ≥.M.S)
        mass = 0.0
        for (bs, b) in ‚Ñ≥.T[s][length(A)]
            for (bs‚Ä≤, b‚Ä≤) in ‚Ñ≥.T[bs][a]
                if bs‚Ä≤ > len
                    continue
                end
                if !haskey(tmp, bs‚Ä≤)
                    tmp[bs‚Ä≤] = 0.0
                end
                tmp[bs‚Ä≤] += b * M.T[bs][a][bs‚Ä≤] * eta(S[bs‚Ä≤])
            end
        end
        for k in keys(tmp)
            mass += tmp[k]
            push!(T, (k, round(tmp[k]; digits=5)))
        end

        mem_p = 1.0-round(mass; digits=5)
        if mem_p != 0.0
            push!(T, (ms‚Ä≤, mem_p))
        end
    end
    return T
end

function generate_reward(‚Ñ≥::SOMDP, s::Int, a::Int)
    M, S, A = ‚Ñ≥.M, ‚Ñ≥.S, ‚Ñ≥.A
    state, action = S[s], A[a]
    if state.state.x == -1
        return -10
    elseif action.value == "QUERY"
        # if length(state.action_list) == ‚Ñ≥.Œ¥
        #     return 0.0
        # end
        return -3.0
    elseif length(state.action_list) == 0
        return M.R[s][a]
    else
        r = 0.0
        for (bs, b) in ‚Ñ≥.T[s][length(A)]
            r += b * ‚Ñ≥.M.R[bs][a]
        end
        return r
    end
end

function generate_heuristic(‚Ñ≥::SOMDP, V::Vector{Float64}, s::Int, a::Int)
    M, S, A = ‚Ñ≥.M, ‚Ñ≥.S, ‚Ñ≥.A
    state, action = S[s], A[a]
    if state.state.x == -1
        return 0.
    end
    if length(state.action_list) == 0
        return V[s]
    else
        h = 0.0
        for (bs, b) in ‚Ñ≥.T[s][length(A)]
            h += b * V[bs]
        end
        return h
    end
    return 0.
end

function generate_successor(‚Ñ≥::SOMDP,
                         state::MemoryState,
                        action::MemoryAction)::MemoryState
    thresh = rand()
    p = 0.
    T = ‚Ñ≥.T[‚Ñ≥.Sindex[state]][‚Ñ≥.Aindex[action]]
    for (s‚Ä≤, prob) ‚àà T
        p += prob
        if p >= thresh
            return ‚Ñ≥.S[s‚Ä≤]
        end
    end
end

function generate_successor(‚Ñ≥::SOMDP,
                             s::Integer,
                             a::Integer)::Integer
    thresh = rand()
    p = 0.
    T = ‚Ñ≥.T[s][a]
    for (s‚Ä≤, prob) ‚àà T
        p += prob
        if p >= thresh
            return s‚Ä≤
        end
    end
end

# function simulate(‚Ñ≥::SOMDP,
#                    ùí±::ValueIterationSolver)
#     M, S, A, R, state = ‚Ñ≥.M, ‚Ñ≥.S, ‚Ñ≥.A, ‚Ñ≥.R, ‚Ñ≥.s‚ÇÄ
#     true_state, G = M.s‚ÇÄ, M.G
#     rewards = Vector{Float64}()
#     for i = 1:10
#         episode_reward = 0.0
#         while true_state ‚àâ G
#             if length(state.action_list) > 0
#                 cum_cost += 3
#                 state = MemoryState(true_state, Vector{CampusAction}())
#             else
#                 s = ‚Ñ≥.Sindex[state]
#                 true_s = index(true_state, M.S)
#                 a = ùí±.œÄ[true_s]
#                 action = M.A[a]
#                 memory_action = MemoryAction(action.value)
#                 cum_cost += M.C[true_s][a]
#                 state = generate_successor(‚Ñ≥, state, memory_action)
#                 if length(state.action_list) == 0
#                     true_state = state.state
#                 else
#                     true_state = generate_successor(M, true_s, a)
#                 end
#             end
#         end
#     end
#     println("Average cost to goal: $cum_cost")
# end

function simulate(‚Ñ≥::SOMDP,
                   ùí±::ValueIterationSolver,
                   ùíÆ::Union{LAOStarSolver,FLARESSolver},
                   m::Int,
                   v::Bool)
    M, S, A, R = ‚Ñ≥.M, ‚Ñ≥.S, ‚Ñ≥.A, ‚Ñ≥.R
    r = Vector{Float64}()
    # println("Expected cost to goal: $(‚Ñí.V[index(state, S)])")
    for i ‚àà 1:m
        state, true_state = ‚Ñ≥.s‚ÇÄ, M.s‚ÇÄ
        episode_reward = 0.0
        while true
            s = ‚Ñ≥.Sindex[state]
            a = ùíÆ.œÄ[s]
            action = A[a]
            if v
                println("Taking action $action in memory state $state in true state $true_state.")
            end
            if action.value == "QUERY"
                # if length(state.action_list) < ‚Ñ≥.Œ¥
                #     episode_reward -= 3.0
                # end
                state = MemoryState(true_state, Vector{DomainAction}())
                episode_reward -= 3
            else
                true_s = index(true_state, M.S)
                episode_reward += M.R[true_s][a]
                state = generate_successor(‚Ñ≥, state, A[a])
                if length(state.action_list) == 0
                    true_state = state.state
                else
                    true_state = generate_successor(M, true_s, a)
                end
            end
            # println("Episode reward so far: $episode_reward")

            if terminal(‚Ñ≥, state) || terminal(true_state, ‚Ñ≥.M.g)
                if v
                    println("Terminating in state $state and true state $true_state.")
                end
                break
            end
        end
        push!(r, episode_reward)
        # println("Episode $i || Total cumulative reward:
        #              $(mean(episode_reward)) ‚®¶ $(std(episode_reward))")
    end
    # println("Reached the goal.")
    println("Total cumulative reward: $(mean(r)) ‚®¶ $(std(r))")
end
#
# function simulate(‚Ñ≥::SOMDP,
#                    ùí±::ValueIterationSolver,
#                    œÄ::MCTSSolver)
#     M, S, A, R = ‚Ñ≥.M, ‚Ñ≥.S, ‚Ñ≥.A, ‚Ñ≥.R
#     rewards = Vector{Float64}()
#     # println("Expected cost to goal: $(‚Ñí.V[index(state, S)])")
#     for i=1:1
#         state, true_state = ‚Ñ≥.s‚ÇÄ, M.s‚ÇÄ
#         r = 0.0
#         while true
#             # s = index(state, S)
#             # a, _ = solve(‚Ñí, ùí±, ‚Ñ≥, s)
#             action = @time solve(œÄ, state)
#             # action = A[a]
#             println("Taking action $action in memory state $state in true state $true_state.")
#             if action.value == "QUERY"
#                 if length(state.action_list) < ‚Ñ≥.Œ¥
#                     r  -= 3
#                 end
#                 state = MemoryState(true_state, Vector{DomainAction}())
#                 # r -= 3
#             else
#                 true_s = index(true_state, M.S)
#                 a = index(action, A)
#                 r += M.R[true_s][a]
#                 state = generate_successor(‚Ñ≥, state, action)
#                 if length(state.action_list) == 0
#                     true_state = state.state
#                 else
#                     true_state = generate_successor(M, true_s, a)
#                 end
#                 print(r)
#             end
#             if terminal(‚Ñ≥, state) || terminal(true_state, ‚Ñ≥.M.g)
#                 println("Terminating in state $state and true state $true_state.")
#                 break
#             end
#         end
#         push!(rewards, r)
#         # println("Episode $i  Total cumulative cost: $(mean(costs)) ‚®¶ $(std(costs))")
#     end
#     # println("Reached the goal.")
#     println("Average reward: $(mean(costs)) ‚®¶ $(std(costs))")
# end

function build_model(M::MDP,
                     Œ¥::Int)
    S, s‚ÇÄ = generate_states(M, Œ¥)
    A = generate_actions(M)
    T = Dict{Int, Dict{Int, Vector{Tuple{Int, Float64}}}}()
    ‚Ñ≥ = SOMDP(M, S, A, T, generate_reward, s‚ÇÄ, Œ¥, generate_heuristic)
    generate_transitions(‚Ñ≥)
    println("Checking transition validity")
    check_transition_validity(‚Ñ≥)
    return ‚Ñ≥
end

function build_models(M::MDP,
                 DEPTHS::Vector{Int})
    MODELS = Vector{SOMDP}()
    A = generate_actions(M)
    T = Dict{Int, Dict{Int, Vector{Tuple{Int, Float64}}}}()
    S, s‚ÇÄ = generate_states(M, 1)
    println(">>>> Building SOMDP for depth Œ¥ = 1 <<<<")
    ‚Ñ≥ = SOMDP(M, S, A, T, generate_reward, s‚ÇÄ, 1, generate_heuristic)
    println(">>>> Total States: $(length(S)) <<<<")
    generate_transitions(‚Ñ≥, false)
    push!(MODELS, ‚Ñ≥)
    tmp_‚Ñ≥ = ‚Ñ≥
    for Œ¥ in DEPTHS
        println(">>>> Building SOMDP for depth Œ¥ = $Œ¥ <<<<")
        S, s‚ÇÄ = generate_states(M, Œ¥)
        println(">>>> Total states: $(length(S)) <<<<")
        ‚Ñ≥ = SOMDP(M, S, A, copy(tmp_‚Ñ≥.T), generate_reward, s‚ÇÄ, Œ¥, generate_heuristic)
        @time generate_transitions(‚Ñ≥, true)
        push!(MODELS, ‚Ñ≥)
        tmp_‚Ñ≥ = ‚Ñ≥
    end
    return MODELS
end

function solve_model(‚Ñ≥, ùí±, solver)
    S, s = ‚Ñ≥.S, ‚Ñ≥.s‚ÇÄ
    println("Solving...")

    if solver == "laostar"
        ‚Ñí = LAOStarSolver(100000, 1000., 1.0, .001, Dict{Integer, Integer}(),
            zeros(length(‚Ñ≥.S)), zeros(length(‚Ñ≥.S)),
            zeros(length(‚Ñ≥.S)), zeros(length(‚Ñ≥.A)))
        a, total_expanded = @time solve(‚Ñí, ùí±, ‚Ñ≥, index(s, S))
        println("LAO* expanded $total_expanded nodes.")
        println("Expected reward: $(‚Ñí.V[index(s,S)])")
        return ‚Ñí
    elseif solver == "uct"
        ùí∞ = UCTSolver(zeros(length(‚Ñ≥.S)), Set(), 1000, 100, 0)
        a = @time solve(ùí∞, ùí±, ‚Ñ≥)
        println("Expected reward: $(ùí∞.V[index(s, S)])")
        return ùí∞
    elseif solver == "mcts"
        U(state) = maximum(generate_heuristic(‚Ñ≥, ùí±.V, state, action)
                                                  for action in ‚Ñ≥.A)
        U(state, action) = generate_heuristic(‚Ñ≥, ùí±.V, state, action)
        œÄ = MCTSSolver(‚Ñ≥, Dict(), Dict(), U, 20, 100, 100.0)
        a = @time solve(œÄ, s)
        println("Expected reard: $(œÄ.Q[(s, a)])")
        return œÄ, a
    elseif solver == "flares"
        ‚Ñ± = FLARESSolver(100000, 2, false, false, -1000, 0.001,
                         Dict{Integer, Integer}(),
                         zeros(length(‚Ñ≥.S)),
                         zeros(length(‚Ñ≥.S)),
                         Set{Integer}(),
                         Set{Integer}(),
                         zeros(length(‚Ñ≥.A)))
        a, num = @time solve(‚Ñ±, ùí±, ‚Ñ≥, index(s, S))
        println("Expected reward: $(‚Ñ±.V[index(s, S)])")
        return ‚Ñ±
    end
end

function solve(‚Ñ≥, ùí±, solver::String)
    if solver == "laostar"
        return solve_model(‚Ñ≥, ùí±, solver)
    elseif solver == "uct"
        return solve_model(‚Ñ≥, ùí±, solver)
    elseif solver == "mcts"
        return solve_model(‚Ñ≥, ùí±, solver)
    elseif solver == "flares"
        return solve_model(‚Ñ≥, ùí±, solver)
    else
        println("Error.")
    end
end

# This is for Connor's benefit running in IDE

function run_somdp()
    ## PARAMS
    MAP_PATH = joinpath(@__DIR__, "..", "maps", "two_buildings.txt")
    SOLVER = "laostar"
    SIM = false
    SIM_COUNT = 100
    VERBOSE = false
    DEPTH = 4
    INIT = 's'
    GOAL = 'g'


    ## MAIN SCRIPT
    begin
        println("Building MDP...")
        M = build_model(MAP_PATH, INIT, GOAL)
        println(">>>> Number of states: $(length(M.S)) <<<<")
        println("Solving MDP...")
        ùí± = solve_model(M)
        println("Building SOMDP...")
        ‚Ñ≥ = @time build_model(M, DEPTH)
        println("Solving SOMDP...")
        solver = solve(‚Ñ≥, ùí±, SOLVER)
    end

    if SIM
        println("Simulating...")
        simulate(‚Ñ≥, ùí±, solver, SIM_COUNT, VERBOSE)
    end
end

function reachability(‚Ñ≥::SOMDP, Œ¥::Int, ùíÆ::LAOStarSolver)
    S, state‚ÇÄ, A, T = ‚Ñ≥.S, ‚Ñ≥.s‚ÇÄ, ‚Ñ≥.A, ‚Ñ≥.T
    s = index(state‚ÇÄ, S)
    œÄ = ùíÆ.œÄ

    denominator = 0
    state_stack = Vector{Int}()
    visited_states = Set{Int}()
    push!(state_stack, s)
    while !isempty(state_stack)
        s = pop!(state_stack)
        push!(visited_states, s)
        if length(S[s].action_list) == ‚Ñ≥.Œ¥
            denominator += 1
        end
        for (a, action) in enumerate(A)
            for (sp, p) in T[s][a]
                if sp ‚àâ visited_states
                    push!(state_stack, sp)
                end
            end
        end
    end

    reachable = Set{Int}()
    reachable_states = Set{MemoryState}()
    reachable_max_depth = 0
    reachable_max_depths = zeros(Œ¥)
    visited = Vector{Int}()
    push!(visited, s)
    while !isempty(visited)
        s = pop!(visited)
        if s ‚àà reachable
            continue
        end
        push!(reachable, s)
        push!(reachable_states, S[s])
        if length(S[s].action_list) > 0
            reachable_max_depths[length(S[s].action_list)] += 1
            # push!(reachable_max_depths[Œ¥], s)
            if length(S[s].action_list) == Œ¥
                reachable_max_depth += 1
            end
        end
        if terminal(‚Ñ≥, S[s])
            continue
        end
        if !haskey(œÄ, s)
            continue
        end
        a = œÄ[s]
        for (s‚Ä≤, p) in T[s][a]
            push!(visited, s‚Ä≤)
        end
    end
    # count = 0
    # for (s, state) in enumerate(S)
    #     if length(state.action_list) == Œ¥
    #         count += 1
    #     end
    # end

    println("Reachable max depth states under optimal policy: $reachable_max_depths")
    # println("Percent of total max depth states reachable under optimal policy: $(length(reachable_max_depth)/(length(S) * (length(A)^Œ¥)))")
    println("Percent of total max depth states reachable under optimal policy: $(100.0*length(reachable_max_depth)/denominator)")
    return reachable_states
end

function action_change_experiment(‚Ñ≥‚ÇÅ, ‚Ñ≥‚ÇÇ, ùíÆ‚ÇÅ, ùíÆ‚ÇÇ)
    S1, S2 = ‚Ñ≥‚ÇÅ.S, ‚Ñ≥‚ÇÇ.S

    non_max_term = Set{MemoryState}()
    for (s, state) in enumerate(S1)
        if length(state.action_list) != ‚Ñ≥‚ÇÅ.Œ¥-1 || !haskey(ùíÆ‚ÇÅ.œÄ, s)
            continue
        end
        a = ùíÆ‚ÇÅ.œÄ[s]
        terminal = true
        for (sp, p) in ‚Ñ≥‚ÇÅ.T[s][a]
            if length(S1[sp].action_list) == ‚Ñ≥‚ÇÅ.Œ¥
                terminal = false
            end
        end
        if terminal == true
            push!(non_max_term, state)
        end
    end

    bad_no_good_counterexamples = Set{MemoryState}()
    for state in non_max_term
        s = index(state, S2)
        a = ùíÆ‚ÇÇ.œÄ[s]
        for (sp, p) in ‚Ñ≥‚ÇÅ.T[s][a]
            if length(S2[sp].action_list) == ‚Ñ≥‚ÇÅ.Œ¥
                push!(bad_no_good_counterexamples, state)
                break
            end
        end
    end

    println("Number of counterexamples is: $(length(bad_no_good_counterexamples))")
end

function run_experiment_script()
    ## PARAMS
    MAP_PATH = joinpath(@__DIR__, "..", "maps", "two_buildings.txt")
    SOLVERS = ["laostar"]
    HEURISTICS = ["vstar", "null"]
    SIM_COUNT = 100
    VERBOSE = false
    ## delta = 1 is always done by default so don't add here.
    DEPTHS = [2,3,4]
    INIT = 's'
    GOAL = 'g'


    println("Building MDP...")
    M = build_model(MAP_PATH, INIT, GOAL)
    println("String buffer")
    println("Number of states: $(length(M.S))")
    println("Solving MDP...")
    ùí± = solve_model(M)
    println(index(M.s‚ÇÄ, M.S))
    println("Expected reward in base domain: $(ùí±.V[index(M.s‚ÇÄ, M.S)])")
    println("Building SOMDPs...")
    MODELS = build_models(M, DEPTHS)
    println("Solving and Evaluating SOMDPS...")
    to = TimerOutput()
    solvers = Vector{Union{FLARESSolver, LAOStarSolver}}()
    for solver in SOLVERS
        for i=1:length(MODELS)
            model = MODELS[i]
            println("\n", ">>>> Solving SOMDP with depth Œ¥ = $(model.Œ¥) <<<<")
            ## TODO: Line below needs to be adjust eventually when we add in
            #        iterating over the different heuristics.
            label = solver * " | " * string(model.Œ¥)
            ùíÆ = @timeit to label solve(model, ùí±, solver)
            # if i > 1
            #     action_change_experiment(MODELS[i-1], model, last(solvers), ùíÆ)
            # end
            push!(solvers, ùíÆ)

            # println("\n", ">>>> Evaluating with depth = $(model.Œ¥) and solver = $solver <<<<")
            # simulate(model, ùí±, ùíÆ, SIM_COUNT, VERBOSE)

            reachability(model, model.Œ¥, ùíÆ)

        end
    end

    show(to, allocations = false)
end

run_experiment_script()
#
run_somdp()
